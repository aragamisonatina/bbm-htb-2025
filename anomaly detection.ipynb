{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac4d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collected 55 edits in the last 15 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10079/3636650258.py:144: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  time_edits = pd.concat([time_edits,new_row])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "✅ Collected 26 edits in the last 15 seconds.\n",
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "✅ Collected 11 edits in the last 15 seconds.\n",
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "✅ Collected 26 edits in the last 15 seconds.\n",
      "✅ Collected 33 edits in the last 15 seconds.\n",
      "✅ Collected 32 edits in the last 15 seconds.\n",
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "✅ Collected 37 edits in the last 15 seconds.\n",
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "✅ Collected 27 edits in the last 15 seconds.\n",
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "⚠️ stream error: 'int' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from requests_sse import EventSource\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "url= 'https://stream.wikimedia.org/v2/stream/mediawiki.recentchange'\n",
    "\n",
    "# Adding headers can help in case the server requires specific request formatting\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Function to determine if the change is to a talk page\n",
    "def is_talk_page(title):\n",
    "    # Typically, talk pages start with \"Talk:\" or \"<Language> talk:\"\n",
    "    # This will handle \"Talk:\", \"User talk:\", \"Wikipedia talk:\", etc.\n",
    "    return any(title.lower().startswith(prefix) for prefix in ['talk:', 'wikipedia talk:', 'file talk:', \n",
    "                                                              'template talk:', 'help talk:', 'category talk:', 'portal talk:',\n",
    "                                                              'book talk:', 'draft talk:', 'timedtext talk:', 'module talk:'])\n",
    "# Helper function to clean the talk prefix\n",
    "def clean_talk_prefix(title):\n",
    "    prefixes = ['talk:', 'wikipedia talk:', 'file talk:', 'template talk:',\n",
    "                'help talk:', 'category talk:', 'portal talk:', 'book talk:',\n",
    "                'draft talk:', 'timedtext talk:', 'module talk:']\n",
    "    title_lower = title.lower()\n",
    "    for prefix in prefixes:\n",
    "        if title_lower.startswith(prefix):\n",
    "            return title[len(prefix):].strip()  # Remove prefix and extra spaces\n",
    "    return title\n",
    "\n",
    "data_list= [[],[],[]]\n",
    "t_end = time.time() + 60\n",
    "# Setting up the EventSource connection\n",
    "\n",
    "with EventSource(url, headers=headers) as stream:\n",
    "    for event in stream:\n",
    "        if time.time() > t_end:\n",
    "            break\n",
    "\n",
    "        if event.type == 'message':\n",
    "            try:\n",
    "                # Parse the event data as JSON\n",
    "                change = json.loads(event.data)\n",
    "                # Check if the change is related to a talk page from Wikipedia\n",
    "            \n",
    "                if change['wiki'].endswith('wiki') and is_talk_page(change['title']) and change['bot'] == False and change['wiki']=='enwiki':\n",
    "                    #get the number of bytes\n",
    "                    old_len = change.get('length', {}).get('old') #old bytes\n",
    "                    new_len = change.get('length', {}).get('new') #edited bytes\n",
    "                    if old_len is not None and new_len is not None:\n",
    "                        byte_diff = abs(new_len - old_len)\n",
    "                    else:\n",
    "                        byte_diff = None #in case no data\n",
    "                \n",
    "                    print('{user} edited {title}: {comment} with {byte_diff} bytes'.format(\n",
    "                        user=change['user'], title=clean_talk_prefix(change['title']), comment = change['comment'], byte_diff = byte_diff))\n",
    "                    data_list[0].append(clean_talk_prefix(change['title']))\n",
    "                    data_list[1].append(change['comment'])\n",
    "                    data_list[2].append(byte_diff)\n",
    "                   \n",
    "                    data = {\n",
    "                        \"Title\": data_list[0],\n",
    "                        \"Comment\": data_list[1],\n",
    "                        \"Bytes\": data_list[2]\n",
    "                    }\n",
    "            \n",
    "            except ValueError: \n",
    "                # In case of any issues in parsing JSON data\n",
    "                continue\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('edit.csv',index = False)\n",
    "'''\n",
    "BATCH_SECONDS = 15\n",
    "STREAM_URL = 'https://stream.wikimedia.org/v2/stream/mediawiki.recentchange'\n",
    "def collect_for(seconds=BATCH_SECONDS):\n",
    "    \"\"\"Collect ANY recent changes for a fixed time window (nearly no filtering).\"\"\"\n",
    "    headers = {\"User-Agent\": \"HTB-Headlines/1.0 (demo)\", \"Accept\": \"text/event-stream\"}\n",
    "    edits = []\n",
    "    start = time.time()\n",
    "    while time.time() - start < seconds:  # ✅ keep trying until full 60s\n",
    "        try:\n",
    "            with EventSource(STREAM_URL, headers=headers) as stream:\n",
    "                for event in stream:\n",
    "                    if time.time() - start >= seconds:\n",
    "                        break\n",
    "                    if event.type != \"message\" or not event.data:\n",
    "                        continue\n",
    "                    try:\n",
    "                        change = json.loads(event.data)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                    if not isinstance(change, dict):\n",
    "                        continue\n",
    "                    if change.get(\"wiki\") != \"enwiki\":\n",
    "                        continue\n",
    "\n",
    "                    title = str(change.get(\"title\", \"\") or \"\").strip()\n",
    "                    comment = str(change.get(\"comment\", \"\") or \"\").strip()\n",
    "                    delta = _size_delta(change)\n",
    "\n",
    "                    edits.append({\n",
    "                        \"user\": change.get(\"user\", \"\"),\n",
    "                        \"title\": title,\n",
    "                        \"comment\": comment,\n",
    "                        \"timestamp\": change.get(\"timestamp\", 0),\n",
    "                        \"delta\": int(delta),\n",
    "                    })\n",
    "\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ stream error: {e}\")\n",
    "\n",
    "    return pd.DataFrame(edits, columns=[\"user\",\"title\",\"comment\",\"timestamp\",\"delta\"])\n",
    "\n",
    "def _size_delta(change: dict) -> int:\n",
    "    \"\"\"Return absolute byte change for the edit if available, else 0.\"\"\"\n",
    "    length = change.get(\"length\") or {}\n",
    "    old = length.get(\"old\"); new = length.get(\"new\")\n",
    "    if isinstance(old, int) and isinstance(new, int):\n",
    "        return abs(new - old)\n",
    "    rev = change.get(\"revision\") or {}\n",
    "    osz = (rev.get(\"old\") or {}).get(\"size\")\n",
    "    nsz = (rev.get(\"new\") or {}).get(\"size\")\n",
    "    if isinstance(osz, int) and isinstance(nsz, int):\n",
    "        return abs(nsz - osz)\n",
    "    return 0\n",
    "\n",
    "time_edits = pd.DataFrame(columns=[\"Minute\",\"Number of edits\",\"Anomaly\"])\n",
    "minute = BATCH_SECONDS/60\n",
    "#continuous loop\n",
    "\n",
    "\n",
    "while True:\n",
    "    batch_df = collect_for(BATCH_SECONDS)\n",
    "    print(f\"✅ Collected {len(batch_df)} edits in the last {BATCH_SECONDS} seconds.\") \n",
    "    new_row = pd.DataFrame({'Number of edits': [len(batch_df)], 'Minute': [minute], 'Anomaly': [0]})\n",
    "    time_edits = pd.concat([time_edits,new_row])\n",
    "    minute = minute+0.25\n",
    "\n",
    "    time_edits.to_csv(\"time_edits.csv\", index=False)\n",
    "    if len(time_edits) > 1: \n",
    "        X = time_edits[['Minute','Number of edits']]\n",
    "        model = IsolationForest(contamination=0.05, random_state=42)\n",
    "        model.fit(X)\n",
    "        scores = model.decision_function(X)\n",
    "        outliers = np.argwhere(scores < np.percentile(scores, 5)).flatten()\n",
    "        time_edits['Anomaly'] = 0\n",
    "        time_edits.iloc[outliers, time_edits.columns.get_loc('Anomaly')] = 1\n",
    "\n",
    "\n",
    "#time analysis using sklearn.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "time_edits = pd.read_csv(\"time_edits.csv\")\n",
    "X = time_edits[['Minute','Number of edits']]\n",
    "# Define the model and set the contamination level\n",
    "model = IsolationForest(contamination=0.05)\n",
    "model.fit(X)\n",
    "scores = model.decision_function(X)\n",
    "# Identify the points with the highest outlier scores\n",
    "outliers = np.argwhere(scores < np.percentile(scores, 5)).flatten()\n",
    "# Plot anomly\n",
    "colors=['green','red']\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if i not in outliers:\n",
    "        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly\n",
    "    else:\n",
    "        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly\n",
    "plt.xlabel('Minutes',fontsize=13)\n",
    "plt.ylabel('Number of Edits',fontsize=13)        \n",
    "plt.title('Anomaly by Isolation Forest',fontsize=16)        \n",
    "plt.show()\n",
    "\n",
    "time_edits['Anomaly'] = 0\n",
    "time_edits.loc[outliers, 'Anomaly'] = 1\n",
    "time_edits.to_csv(\"time_edits_with_anomaly.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdb026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b1534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
